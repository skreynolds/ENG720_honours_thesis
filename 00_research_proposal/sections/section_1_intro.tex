\section{Introduction \& Background}
In 2018 there was approximately 261$\si{\tera\watt\hour}$ of power generation in the Australian electricity sector. Renewables contributed to 19\% of the total generation, increasing from 15\% in 2017. The Department of Industry, Science, Energy and Resources have noted a small trend of increasing renewable energy penetration year on year in the electricity generation market since 2008, as shown in Figure \ref{fig:energyts} \cite{Diser2020}.
\begin{figure}[ht]
	\centering
	\includegraphics[height=7cm]{australian_generation_profile}
	\caption{Timeseries of Terrawatt hours of generation for renewable sources of generation, and total sources of generation in Australia from 1977 to 2018.}
	\label{fig:energyts}
\end{figure}

One of the benefits of transitioning from thermal sources of generation to renewable sources is reduced greenhouse gas emissions \cite{IPCC2012}, however, this transition is not without its drawbacks. Increased reliance on renewable generation sources poses some challenges regarding power system stability. A recent example is the system failure resulting from an event cascade, triggered by cloud cover shadowing a solar array in Alice Springs. The system failure resulted in a blackout occurring in Alice Springs for approximately 8 hours \cite{UCNT2019}. The Entura report highlighted that poor control policies were one of the many factors contributing to the blackout. In this instance, a generator provisioned to ramp up in the event of cloud cover limiting solar array output was unable to be controlled. Moreover, generators that were still under the control regime were issued operating set points above their rated capacity, which eventually resulted in thermal overload and subsequent tripping from the protection system \cite{Wilkey2019}.

Currently employed control methods use classical feedback loop techniques. They can be brittle when faced with system changes, or scenarios which they were not designed for. A better controller would be one that can learn and adapt it's control system to an unseen system or event. to a system, given some broad control objective. This research proposes a Deep Reinforcement Learning agent for controlling the frequency of a power system consisting of multiple generators, and multiple load demands with stochastic profiles.

\subsection{Power Systems and Frequency}
Interconnected power systems are comprised of power generating units and energy storage systems connected to transmission and distribution networks. Generated power is used to service load demand. A single line diagram of a power network can be seen in Figure \ref{fig:generation}. The left hand side of the diagram shows thermal generation units, such as coal and nuclear, in addition to renewable sources of generation, like wind and solar. The right hand side of the figure shows the distribution network and the consumers of generated energy: industry and households.
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.85]{power_system}
	\caption{A single line diagram of a typical power system taken from \cite{Glavic2019}. The image shows points of generation from thermal and renewable sources, and the subsequent supply of generated energy to meet load demand through the transmission and distribution network.}
	\label{fig:generation}
\end{figure}

One of the key elements to successful operation of interconnected power systems is ensuring total load demand is matched with total generation while taking into account power losses involved with generation, transmission, and distribution \cite{Wood2013}. To understand why it is important to match generation with load demand it is useful to first consider the basic operation of a single thermal generator. 
\begin{figure}[h]
\centering
\includegraphics[height=1.4cm]{generation}
\caption{A thermal generation unit consists of a prime mover (turbine), and a synchronous machine. This image was taken from \cite{Wood2013}.}
\label{fig:turbine}
\end{figure}

The essential elements of a thermal generator are a prime mover (such as a gas turbine) and a synchronous machine, as depicted in Figure \ref{fig:turbine}. The prime mover provides mechanical torque, $T_{mech}$, which drives the synchronous machine producing electrical energy. In response, the synchronous machine creates an opposing torque which depends on the size of the load demand from households and industry. This opposing torque is referred to as electrical torque and is denoted as $T_{elec}$. If $\alpha$ represents angular acceleration of the generator rotating mass, and $I$ is its moment of inertia, then by Newton's second law:
\begin{equation}
\sum T_i = I \alpha \label{eq:1}
\end{equation}

Equation \ref{eq:1} shows that when $T_{mech}$ equals $T_{elec}$ the system will be in a steady state of zero angular acceleration with a constant angular velocity $\omega$. Now, if $T_{mech} > T_{elec}$, then the system has some angular acceleration causing the angular velocity $\omega$ to increase. This results in a frequency increase in the system. Conversely, if $T_{mech} < T_{elec}$ then the angular velocity $\omega$ will decrease, resulting in a frequency decrease. What makes this situation interesting is that, at any point in time, the total electrical load demand will fluctuate stochastically as businesses and households switch grid connected devices or motors on and off. The implication is that an uncontrolled system will have a continually changing frequency. Australia's electricity network is designed to operate at a frequency of 50$\si{\hertz}$. In the majority of network scenarios the Australian Energy Market Operator (AEMO) has a desired operating range for frequency which lies between 49.85$\si{\hertz}$ and 50.15$\si{\hertz}$ \cite{AEMOfreqdev}. Similarly, the Power and Water Corporation (PWC) Network Technical Code for the Northern Territory states that under normal operating conditions frequency should be maintained in the range 49.80$\si{\hertz}$ to 50.20$\si{\hertz}$ \cite{Pwc2013}. Operation outside of specified ranges can cause damage to electrical equipment such as transformers or motors, which are designed to operate at specific frequencies \cite{Sen2014}. Network designers engineer protection schemes so that sustained frequency excursions outside of the allowed range will cause equipment to trip from the network \cite{AEMOpowerfreqriskrev}.

\begin{figure}[ht]
	\centering
	\includegraphics[height=7cm]{load_profile}
	\caption{Intraday timeseries of weekday energy demand profile in South Australia during summer \cite{Aemosaenergyrep}.}
	\label{fig:energydemand}
\end{figure}

Protection schemes tripping equipment from the network is undesirable since this can leave households and industry without power, resulting in economic loss. Further, if disconnections are uncontrolled then this can reduce system stability further \cite{AEMOpowerfreqriskrev}. System controllers, such as the AEMO and PWC, are therefore interested in being able to control the system to follow changes in load demand so that system frequency is maintained in the allowable range. Additionally, they are interested in control mechanisms to restore frequency excursions as a result of unexpected disturbances. System controllers can use historical data, like that shown in Figure \ref{fig:energydemand}, to forecast daily demand profiles with some reliability. This type of forecasting does not help when trying to predict the occurrence of random disturbances, however, it does provide a starting point for estimating required generation needed to meet demand. It is important to note that forecasting is not perfect. Inevitably mismatches in supply and demand will occur causing small imbalances between $T_{mech}$ and $T_{elec}$, resulting in a change to angular velocity $\omega$ and the network frequency \cite{Glover2012}. To perfectly match supply and demand, system controllers use generators referred to as regulating units \cite{Kothari2011}. A regulating unit is a generator that has capacity to increase or decrease mechanical torque $T_{mech}$. If the system controller has a sufficient number of regulating units it can perform two functions: load following; and restoring the system to stable operating conditions in the event of a disturbance \cite{Grainger1994}. Using a regulating unit to load follow is referred to as the provision load following ancillary services \cite{AEMOancilliaryserv}. Load following control adjusts regulating units slightly to match supply perfectly with a demand load profile, like that shown in Figure \ref{fig:energydemand}. Using a regulator to restore the system after a disturbance is referred to as providing spinning reserves \cite{AEMOancilliaryserv}.

\begin{figure}[ht]
\centering
\includegraphics[height=8cm]{frequency_arrest}
\caption{A frequency disturbance occurs just before the 10 second mark, and regulating units ramp up their generation to first arrest the disturbance, and provide the subsequent correction, returning system frequency to 50$\si{\hertz}$.}
\label{fig:freqarrest}
\end{figure}

When used in this fashion it is important to note that the regulating unit is not responsible for arresting frequency excursions, rather, it is used to restore the system back to the allowable frequency operating range after the frequency excursion has been arrested. An example of a frequency excursion, arrest, and subsequent restoration can be seen in Figure \ref{fig:freqarrest}. AEMO and PWC do not require all generators on the network to act as regulating units since adequate frequency control can be achieved using a subset of the total available generators.

\subsection{Frequency control for a single area system}\label{oneareapowersystem}
The power system model shown in Figure \ref{fig:energyts} depicts total generation coming from many generation assets - this is complex to model. Researchers often find it useful to divide generation assets into sub-groups referred to as control areas \cite{Kothari2011}. A control area is defined as a subset of generators which are in close proximity to each other and constitute a coherent group that speed up and slow down together, maintaining their relative power angles \cite{Kothari2011}. The total network is therefore comprised of many interconnected control areas. An example of a series of interconnected control areas can be seen in Figure \ref{fig:interconnectedpa}. Notice that for each area there is only one load and one generator. Typically, for each control area, researchers will aggregate many loads into a single load, and many generators into a single generator. This simplifies the model further \cite{Grainger1994}.
\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{multiple_area_system}
	\caption{An example of three interconnected control areas in a 60$\si{\hertz}$ power system. The interconnections allow power to flow from one area to another, allowing generators to service loads from different areas. Each control area is consists of many generators and loads, but are modelled with a single generator and single load, respectively \cite{Grainger1994}.}
	\label{fig:interconnectedpa}
\end{figure}

The simplest power system to control is one that consists of a single control area. A single control area power system has no interconnections to any other control area. It is comprised of consumer load demand, and a set of generators, some of which are acting as regulating units. As previously mentioned, for modelling simplicity, loads are aggregated to a single load, and generators are aggregated to a single generator. This simple system is a classic example and well understood. It is generally acknowledged that a governor feedback control regime can successfully perform frequency control of a single area power system \cite{Wood2013, Grainger1994, Kothari2011}. Most introductory textbooks on power systems cover governor control of this system. A particularly well laid out approach to developing linear models for the turbine, generator load, and governor can be found in \cite{Kothari2011} - the full model is shown in Figure \ref{fig:singleareacontrol}. The leftmost block is a first-order linear model of the speed governor, the second block is a first-order model of the turbine, which the controller directly acts on. The final block is the generator load, which is also a first order system. The over all system model is a second order linear model, with a first order controller.

\begin{figure}[ht]
\centering
\includegraphics[height=5cm]{single_area_control}
\caption{A classical feed back control approach to a second order linear system. The second order system is comprised of a first order model for both the turbine, and generator load. The controller is modelled as a first order system \cite{Kothari2011}.}
\label{fig:singleareacontrol}
\end{figure}


\subsection{Frequency control for two area system}
The single area system presented in Section \ref{oneareapowersystem} is useful to help understand the role of governors in controlling power system frequency, however, a single area model is too simple. In reality, power systems are comprised of many control areas connected by transmission lines (referred to in the literature as tie lines). Often it is the case that there is some net power transfer over the tie lines, enforceable by contract.

\begin{figure}[ht]
	\centering
	\includegraphics[height=3cm]{two_area_system}
	\caption{Two area power system is comprised of generators and load connected via a tie line. Power flows from one area to the other depending on economic contracts.}
	\label{fig:twoareapower}
\end{figure}

Distinct control areas are typically thought of as different participants in the generation market, or simply as different regions in which generation assets are based \cite{Kothari2011}. The simplest model which includes tie lines is the two area power system, shown in Figure \ref{fig:twoareapower}. The control objective with this system is to maintain the inter-area power transfer, whilst regulating the frequency of each area. Simply relying on governor control in each area will not satisfy the control objective. To see this, consider control area A supplying a 50MW load, with a contract to supply control area B with 20MW over the tie line connecting them. If control area B also has a 50MW load, then it is supplying only 30MW to satisfy the demand in this area. Now suppose a 30MW load increase was observed in the demand for area A. Relying on governor control will see generators from both area A and area B speed up in response to this increased load. Ultimately the increased power demands will be met, however, the power transfer over the tie line is likely to be less than the contracted 20MW value, which is problematic. Contract violations due to system instability and control issues do not allow for a stable market in which energy can be reliably traded. Fortunately, two area power systems are well understood. Linear models have been developed to simulate these systems, and classical control approaches have been successfully implemented to meet the new control objectives. In order to achieve this, a metric called Area Control Error (ACE) is used. This measures the distance from a target frequency as well as the deviation from tie-line contractual obligations. The implementation of this control system is shown in Figure \ref{fig:twoareacontrolblock}.

\vspace{2cm}

\begin{figure}[ht]
	\centering
	\includegraphics[height=4.8cm]{two_area_control_block}
	\caption{A classical feed back control approach to a two power area system. Image taken from \cite{Kothari2011}.}
	\label{fig:twoareacontrolblock}
\end{figure}

\newpage

\subsection{Reinforcement learning}
Reinforcement Learning (RL) is a branch of machine learning that is concerned with how agents make sequential decisions to maximise some notion of a cumulative reward. It is a simple idea that allowed Google's Deep Mind to beat the worlds best players in the game of Go \cite{Silver2016}. One of the important aspects of RL is the underlying architecture of the model allows for generalisation to many different applications, albeit the models do require training with different data sets for each application. Subsections 1.4.1 and 1.4.2 provide a brief overview of key architectural components of RL, and subsection 1.4.3 gives details on how these components are implemented to build an agent that can perform a control activity for some application.

\subsubsection{Markov decision process}
Suppose an agent exists in some environment which is comprised of many discrete states, $s \in S$, such that $S$ denotes the state space. At any discrete point in time the agent can take an action $a \in A$, where $A$ denotes the action space. When the agent takes an action in a given state, the agent receives some reward, denoted with $r \in R$, where $R$ is the reward set. If an agent is in a given state, $s$, and takes and action, $a$, this will transition the agent to a new state, $s'$, and yield reward, $r$, with some given probability - these are referred to as state transition probabilities. Transition probabilities are denoted as follows:
\begin{equation}
P(S_{t+1}=s', R_{t+1}=r \ | \ S_t = s, A_t = a)\label{eq:2}
\end{equation}

The set of parameters, outlined above, make up a framework referred to as a Markov Decision Process (MDP) \cite{Bellm1957}. The MDP framework is important to understanding how RL works, however, it must be noted that it is not necessary for the agent to have any information about the state transition dynamics to develop an effective control regime.

\subsubsection{Return, episodes, and policy}
As the robotic agent takes actions at each discrete time step, it receives a reward. The cumulative sum of this reward is referred to as the return \cite{openai2018}. The return is denoted, for $N$ discrete time steps, as:
\begin{equation}
G_t = r_t + r_{1+1} + r_{t+2} + \ldots + r_{N-1}\label{eq:3}
\end{equation}

Often it is convenient to make future rewards less important than more immediate rewards. This is achieved by multiplying each reward in the sequence by a discount factor, $\gamma \in [0,1]$. Equation \ref{eq:3} then becomes:
\begin{equation}
G_t = r_t + \gamma \ r_{1+1} + \gamma\mystrut^2 r_{t+2} + \ldots + \gamma\mystrut^{N-1} r_{N-1} = \sum_{k = 0}^{N-1} \gamma\mystrut^k r_{t+k}
\end{equation}

The duration of time that an agent will cumulate reward is referred to as an episode. An episode is made up of a beginning, middle, and an end. Typically this consists of an RL agent beginning in some initial state. As time passes the agent takes actions, undergoes state transitions, and collects rewards. The episode concludes when the agent reaches a terminal state. At the episode conclusion, the agent receives it's cumulative reward \cite{Kaelbling1996}.

Finally, in order for the robot to act within the environment, it needs to have a policy. A policy, $\pi$, is defined as a mapping from states to actions, that is, a rule which determines what action the robot will take for a given state. A deterministic policy, $\pi (s)$, maps a single action to a single state. A stochastic policy, $\pi (a | s)$, defines a probability distribution over the actions for a given state. An optimal policy, denoted $\pi^*$, is a policy which will maximise the cumulative reward that the agent receives over an episode \cite{Bellm1954}. It can be thought of as the best control policy an agent can have for the given the environment and cumulative reward function.

\subsubsection{How does an RL agent learn?}
The main objective of RL is to develop an optimal policy. There are many algorithmic approaches to building an optimal policy. One of the most common implementations is called Q-Learning. This approach focuses on finding q-values for each state-action pair. A q-value can be thought of as an ordinal value that is discovered and assigned to a state-action pair which tells the agent how important an action is relative to the rest of the actions in a given state. To discover q-values, the agent normally starts with a randomised policy meaning that all the q-values are set to zero. This will lead the agent to take actions at random to explore the state-action space. Higher q-values are assigned to state-action pairs that the agent determines are useful for building a high cumulative reward. Similarly, the agent assigns low q-values to state-action pairs which do not lead to high cumulative rewards. This process is akin to the agent modifying its policy. The q-value modification process is repeated for many episodes and eventually the agent policy converges on an optimal policy. Often the q-values are presented in a tabular format that the literature refers to as a Q-table. An example of a Q-table can be seen in Figure \ref{fig:qtable}. Rows represent different states, and columns represent different actions. Values in each cell provide an ordinance on how valuable each action is for a given state. The agent only needs to understand inputs that uniquely define a state, and the actions it can take in order to learn an optimal policy. It is not necessary for the agent to know the state transition dynamics of the system, described by Equation \ref{eq:2}. More concretely, an agent can learn to control a system for which is does not have a mathematical model.
\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{q_table}
	\caption{The Q-table on the left shows the initialised policy when the agent begins learning. The middle and rightmost Q-tables show the agent developing an understanding of which actions are valuable in which states.(REFERENCE)}
	\label{fig:qtable}
\end{figure}

\subsection{Deep reinforcement learning}
For low dimensional state-action spaces RL approaches result in reasonable control performance, however, as state space dimensionality increases models like Q-tables experience difficulty. The main reason for this is simply that it becomes difficult for the discrete RL algorithm to visit every state action pair resulting in unchanged values for an increasing number of state-action pairs. Essentially this means that the agent does not have complete knowledge of optimal actions for every given state, leading to the derivation of sub-optimal policies.
\begin{figure}[ht]
\centering
\includegraphics[height=5cm]{deep_reinforcement_learning}
\caption{The agent interacts with the environment by taking actions, which affect the state it is in and the reward it receives. The rewards allow agent to adjust the weights in the neural net to build better policies.}
\end{figure}

To get around this problem, for RL problems with high dimensional state spaces, the discrete Q-table is replaced with a function approximator known as a neural network. A high level overview of the architecture can be seen in Figure 11. It is the neural network architecture in which the agent policy is implemented. As the agent learns, it adjusts weights in the neural network to change the policy. This is approach is powerful because neural networks are good at generalising, and hence the agent does not need to visit every state action pair to be able to make good decisions.