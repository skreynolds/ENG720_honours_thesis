\chapter{Discussion and Future Directions}

Reward functions are designed to elicit a desirable control behaviour from the agent. The task of controlling frequency and power interchange for a two area power system should therefore have a reward structure which will incentivise the agent to minimise frequency deviations and minimise deviations in the tie-line power interchange from scheduled values. Moreover, the agent should be discouraged from letting the system see large frequency excursions that would activate system protection settings on a real world power system. Finally, it would be useful if the agent develops policies which meet the objectives outlined above, without requiring extreme control signals.

Whilst there are no restrictions on reward function specification, it is widely acknowledged that poor reward function selection can impede agent learning, or produce undesirable behaviour. Evidence of unstable and divergent learning can be observed in experiments A and B, as shown in subplots (a) and (b), for Figures \ref{fig:results_A} and \ref{fig:results_B}, respectively. This resulted in the DDPG agent developing poor control policies that saturated control actions, shown in subplots (e) and (f). Agent frequency control performance saw unacceptably large deviations from scheduled values, shown in subplots (c) and (d).

Experiments C, D, and E removed early termination conditions and associated penalties, resulting in an improvement to agent learning stability, as shown in subplots (a) and (b), for Figures \ref{fig:results_C}, \ref{fig:results_D}, and \ref{fig:results_E}, respectively. Control policies developed under these conditions no longer saturated control actions, shown in subplots (e) and (f). This led to agent control performance more closely resembling the optimally tuned PI controller, shown in subplots (c) and (d).

After reviewing agent performance during training it was observed that control signals in experiments A through E were noisy (note that these figures have not been included in this report). OU noise is added to control signals in order to help the DDPG agent explore the policy and state-action value spaces to reduce the probability of convergence on a sub-optimal policies; however, excessive noise settings have been shown to impede agent learning during the later stages of training \cite{}. Reducing the OU noise parameters by a factor of 10 in experiment F saw agent frequency control performance closely resemble the frequency control performance of the optimally tuned PI controller, shown in subplots (c) and (d), for Figure \ref{fig:results_F}. Whilst not conclusive, this result provides evidence of the feasibility for using DDPG trained neural networks in controlling the frequency of a two area power system.

