\section{Model Development and Implementation}
Model development is focused on obtaining a mathematical expression for modelling the two area power system and the PI controller, in addition to specifying the neural network architectures for experiments. Implementation primarily focuses on the DDPG training algorithm along with Python class implementations for the environment, PI controllers, and neural networks.

%---------------- SS: Environment

\subsection{Environment Model} \label{ssec:env_modelling}
The two area power system model described in \textsection \ref{ssec:modelling_two_area_system}, and shown in Figure \ref{fig:208_two_area_pi_control_model}, was converted from the frequency domain to the temporal domain. This provides the reinforcement learning architectures, outlined in \textsection \ref{sec:reinforcement_learning}, with the ability to export control signals to the power system at each time step. This approach is common practice when developing environments for reinforcement learning \cite{Brockman2016}. Additionally, frequency domain simulation techniques, such as Laplacian transforms, have strict initial condition assumptions \cite{Ogat2010}, which limit the application of this technology in real world applications.

Higher order ordinary differential equations and systems involving higher order ordinary differential equations provide a more compact system representation; however, to accommodate numerical analysis schemes, such as Runge-Kutta, the environment system was expressed as a system of first-order linear ordinary differential equations.

\begin{figure}[h]
	\centering
	\resizebox{\textwidth}{!}{\input{./figures/4101_two_area_power_system_temporal_model/two_area_power_system_temporal_model}}
	\caption{Variable assignment for a two area power system environment to model in the temporal domain}
	\label{4101_two_area_power_system_temporal_model}
\end{figure}

Suppose variables are assigned to the power system according to Figure \ref{4101_two_area_power_system_temporal_model}. The first order system of linear ordinary differential equations for area 1 is:
\begin{align}
	\dot{x}_2(t) &= \frac{1}{T_{sg_1}}\big( K_{sg_1} u_1(t) - x_2(t) \big) \label{eq:4101} \\
	\dot{x}_3(t) &= \frac{1}{T_{t_1}} \big( K_{t_1} x_2(t) - x_3(t) \big) \label{eq:4102} \\
	\dot{x}_4(t) &= \frac{1}{T_{gl_1}} \bigg( K_{gl_1} \big( x_3(t) - x_5(t) - \Delta p_{L1}(t) \big) - x_4(t) \bigg) \label{eq:4103}  \\
	\dot{x}_5(t) &= 2 \pi T_{12} \big( x_4(t) - x_9(t) \big) \label{eq:4104} \\
	\dot{x}_7(t) &= \frac{1}{T_{sg_2}}\big( K_{sg_2} u_2(t) - x_7(t) \big) \label{eq:4105} \\
	\dot{x}_8(t) &= \frac{1}{T_{t_2}} \big( K_{t_2} x_7(t) - x_8(t) \big) \label{eq:4106} \\
	\dot{x}_9(t) &= \frac{1}{T_{gl_2}} \bigg( K_{gl_2} \big( x_8(t) - x_5(t) - \Delta p_{L2}(t) \big) - x_9(t) \bigg) \label{eq:4107}
\end{align}

A full derivation of the first order linear system described by Equations \ref{eq:4101} to \ref{eq:4107} is described in Appendix \ref{app:C1_power_system_ode}. The equations were implemented as a method \verb|int_power_system_sim| in a Python class \verb|TwoAreaPowerSystemEnv|. Implementation for \verb|int_power_system_sim| is detailed in Appendix \ref{app:implementation_of_power_system_model}.



%---------------- SS: PI Controller

\subsection{Classical PI Controller Model}
The proportional integral (PI) controller model described in \textsection \ref{ssec:control_two_area_system}, and shown in Figure \ref{fig:208_two_area_pi_control_model}, was converted from the frequency domain to the temporal domain to ensure compatibility with the environment model.

\begin{figure}[h]
	\begin{minipage}[b]{0.5\textwidth}
		\resizebox{7.0cm}{!}{\input{./figures/4102_two_area_pi_controller_temporal_1/two_area_pi_controller_temporal_1}}
		\caption{Variable assignment for the PI controller for area 1 in order to model in the temporal domain}
		\label{fig:4102_two_area_pi_controller_temporal_1}
	\end{minipage}
	\hspace{0.1cm}
	\begin{minipage}[b]{0.5\textwidth}
		\resizebox{7.2cm}{!}{\input{./figures/4103_two_area_pi_controller_temporal_2/two_area_pi_controller_temporal_2}}
		\caption{Variable assignment for the PI controller for area 2 in order to model in the temporal domain}
		\label{fig:4103_two_area_pi_controller_temporal_2}
	\end{minipage}
\end{figure}

Suppose variables are assigned to the controller for area 1 and the controller for area 2 according to Figures \ref{fig:4102_two_area_pi_controller_temporal_1} and \ref{fig:4103_two_area_pi_controller_temporal_2}, respectively. The first order system of linear differential equations for the PI controller are:
\begin{align}
	\dot{x}_1(t) &= b_1 \Delta f_1(t) + x_5(t) \label{eq:4108} \\
	\dot{x}_6(t) &= b_2 \Delta f_2(t) - x_5(t) \label{eq:4109}
\end{align}

Note that once the PI controller has stepped forward in time during simulation the actual control signals exported from the controller are $u_1(t)$ and $u_2(t)$. These are expressed as:
\begin{align}
	u_1(t) &= x_1(t) + x_5(t) - R_1 x_4(t) \\
	u_2(t) &= x_6(t) - x_5(t) - R_2 x_9(t) 
\end{align}

A full derivation of the first order linear system described by equations \ref{eq:4108} and \ref{eq:4109} is described in Appendix \ref{sec:temporal_domain_for_pi_controller}. The system of equations is implemented as a method \verb|int_control_system_sim| in a Python class \verb|ClassicalPiController|. Implementation for \verb|int_control_system_sim| is detailed in Appendix \ref{sec:implementation_of_controller}.

%---------------- SS: DDPG Controller

\subsection{DDPG Controller}

%---------------- SSS: Neural Networks

\subsubsection{Neural Networks}
Deep deterministic policy gradient (DDPG), described in \textsection \ref{ssec:deep_deterministic_policy_gradient}, is used to train neural networks to perform control actions given a state observation. DDPG uses actor and critic networks, both of which also have target networks. The implication is that a total of four neural networks are required for a single DDPG controller instance. To accommodate this requirement, two Python classes were created: one for the actor called \verb|Actor|, and another for the critic called \verb|Critic|. Implementations for the \verb|Actor| and \verb|Critic| classes can be found in Appendices \ref{app:implementation_actor_model} and \ref{app:implementation_critic_model}, respectively. Both actor and critic networks were built using an input layer, two hidden layers and an output layer --- an identical structure to experiments conducted by Lillicrap \textit{et al.} \cite{Lillicrap2015}. Neural network hidden layers used rectified linear units (ReLU) for activation functions. The final output layer of the actor network used a $\tanh$ activation function to bound the actions. Both actor and critic architectures contain 121800 weight parameters. The details of the actor and critic networks are shown in Table \ref{tab:4101}.

\begin{table}[h]
	\centering
	\caption{An overview of actor and critic neural network architectures used in experiments undertaken by Lillicrap \textit{et alias}}
	\begin{tabular}{@{\extracolsep{6pt}}llrlr@{}}
		\toprule
		 & \multicolumn{2}{c}{\textbf{Actor}} & \multicolumn{2}{c}{\textbf{Critic}} \\ 
		\cline{2-3} \cline{4-5}
		\multirow{2}{*}{\textbf{Layer}} & \textbf{Activation} & \textbf{Number of} & \textbf{Activation} & \textbf{Number of} \\
		 &  \textbf{Function} & \textbf{Perceptrons} & \textbf{Function} & \textbf{Perceptrons} \\
		\midrule
		Input Layer & None & 3 & None & 3 \\
		Hidden Layer 1 & ReLU & 400 & ReLU & 400 \\
		Hidden Layer 2 & ReLU & 300 & ReLU & 300 \\
		Output Layer & $\tanh$ & 2 & None & 2 \\
		\bottomrule
	\end{tabular}
	\label{tab:4101}
\end{table}

Alternative neural network architectures are also developed for comparison. These alternative actor and critic architectures are common in DDPG control of robotic systems and contain less weight parameters than the Lillicrap \textit{et al.} networks. The actor network contains a total of 1280 weight parameters, and the critic network contains a total of 99200 weight parameters. An additional feature of the critic is that it makes use of Leaky ReLUs to avoid the dying ReLU problem discussed in \textsection \ref{sec:activation_functions}. Implementations for these alternative classes can be found in Appendices \ref{app:implementation_alt_actor_model} and \ref{app:implementation_alt_critic_model} for the actor and critic, respectively. Network details can be found in Table \ref{tab:4102}.

\begin{table}[h]
	\centering
	\caption{An overview of actor and critic neural network architectures seen frequently in the literature for deep reinforcement learning for robotic system control}
	\begin{tabular}{@{\extracolsep{6pt}}llrlr@{}}
		\toprule
		 & \multicolumn{2}{c}{\textbf{Actor}} & \multicolumn{2}{c}{\textbf{Critic}} \\ 
		\cline{2-3} \cline{4-5}
		\multirow{2}{*}{\textbf{Layer}} & \textbf{Activation} & \textbf{Number of} & \textbf{Activation} & \textbf{Number of} \\
		 &  \textbf{Function} & \textbf{Perceptrons} & \textbf{Function} & \textbf{Perceptrons} \\
		\midrule
		Input Layer    & None  & 3   & None  & 3 \\
		Hidden Layer 1 & ReLU  & 256 & LReLU & 256 \\
		Hidden Layer 2 &       &     & LReLU & 256 \\
		Hidden Layer 3 &       &     & LReLU & 128 \\
		Output Layer & $\tanh$ & 2  & None   & 1 \\
		\bottomrule
	\end{tabular}
	\label{tab:4102}
\end{table}

%---------------- SSS: Training

\subsubsection{Training}
Actor and critic neural networks are trained using the DDPG algorithm shown in listing \ref{ssec:deep_deterministic_policy_gradient}. The DDPG algorithm was implemented using a Python class \verb|DdpgController|, and a function called \verb|ddpg_train|.

When an instance of the \verb|DdpgController| class is created a number of critical tasks are performed. These include initialisation of neural networks in memory for the actor and critic, and their associated target networks, declaration of DDPG hyperparameters discussed in \textsection \ref{ssec:deep_deterministic_policy_gradient}, initialisation of a replay buffer for storing agent experience, and the initialisation of an Ornstein–Uhlenbeck process to add exploratory noise to action signals. Additionally, the \verb|DdpgController| class defines methods used for model training. A description of key methods is provided in Table \ref{tab:4103}. The \verb|DdpgController| class implementation can be found in Appendix \ref{app:implementation_ddpg_controller}.

\begin{table}[h]
	\centering
	\cprotect\caption{Description of key methods for the \verb|DdpgController| class}
	\begin{tabular}{lp{12cm}}
		\toprule
		\textbf{Method} & \textbf{Description} \\
		\midrule
		\verb|step| & Stores the current experience tuple, $(s_t, a_t, r_t, S_{t+1})$, in the experience replay buffer, and calls the method \verb|learn|\\
		 & \\
		\verb|act| & Takes a state as input and uses this for a call to the neural network method \verb|forward| which returns an action\\
		 & \\
		\verb|learn| & Performs gradient descent, using backpropagation, on the actor and critic loss functions to adjust weights for each respective network using a set of random uniformly sampled experiences from the experience replay buffer\\
		\bottomrule
	\end{tabular}\label{tab:4103}
\end{table} 

The function \verb|ddpg_train| takes environment and agent class instances as inputs. The function runs training for a specified number of episodes. For each episode, \verb|ddpg_train| takes time steps of 0.01 sec until the episode triggers a termination condition. During each time step, \verb|ddpg_train| will obtain an action from the agent given the current state, obtain the power demand signal for the given time step, and step the environment forward using the agent's action and power demanded. The function the makes a call to the agent method \verb|step|, which stores the experience in the replay buffer and trains the neural network.