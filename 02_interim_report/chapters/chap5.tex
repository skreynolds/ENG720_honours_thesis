\chapter{Preliminary Investigation}
A preliminary investigation was undertaken to assess the DDPG algorithm's capacity to train a neural network tasked with controlling the frequency and power interchange of a two area power system. The preliminary investigation was conducted according to the outline in \textsection \ref{sec:preliminary_investigation}; however, due to time constraints only changes to the environment reward function and OU exploratory noise parameters were considered. A total of 6 experiments were conducted, labelled A through F. Environment parameters were obtained from a two area power system commonly modelled in the literature \cite{}. Similarly, parameters for an optimally tuned PI controller for the given environment were used. Parameter details are shown in Table \ref{tab:5000}.
\begin{table}[h]
	\centering
	\caption{Environment and controller parameters used for preliminary investigation experiments.}
	\begin{tabular}{llr}
		\toprule
		\textbf{Description} & \textbf{Parameter} & \textbf{Value} \\
		\midrule
		Speed Governor Gain & $K_{sg_1}$, $K_{sg_2}$ & 1 \\
		Speed Governor Time Constant & $T_{sg_1}$, $T_{sg_2}$ & 0.08 \\
		Turbine Gain & $K_{t_1}$, $K_{t_2}$ & 1 \\
		Turbine Time Constant & $T_{t_1}$, $T_{t_2}$ & 0.3 \\
		Generator Load Gain & $K_{gl_1}$, $K_{gl_2}$ & 120 \\
		Generator Load Time Constant & $T_{gl_1}$, $T_{gl_2}$ & 20 \\
		Tieline & $T_{12}$ & 0.1 \\
		Proportional Gain & $R_1$, $R_2$ & 2.4 \\
		Integral Gain & $K_{i_1}$, $K_{i_2}$, & -0.671 \\
		 & $b_1$, $b_2$ & 0.425 \\
		\bottomrule
	\end{tabular}\label{tab:5000}
\end{table}

At the commencement of each experiment a new instance of a DDPG agent was initialised, and replay buffers cleared. Each experiment used an identical episode scenario in which the system and agent response was simulated for a total of 30 sec, after which the episode was terminated. During this time a 0.01pu step change in the power demanded for area 1 was introduced at the 1 sec mark, as shown in Figure \ref{fig:5001_demand_profile}.

\begin{figure}[h]
	\centering
	\input{./figures/5001_demand_profile/demand_profile.tikz}
	\caption{At the 1 sec mark a the system experiences a step load change in the power demanded in Area 1, and the simulation continues for 30 sec thereafter.}
	\label{fig:5001_demand_profile}
\end{figure}

\newpage

From initialisation the simulation was incrementally stepped forward by 0.01 sec for a total of 3000 steps. At each time step the DDPG agent was trained using experience stored in the replay buffer from current and previous system interactions, for a given experiment. Experiment were allowed to run for a total of 200 episodes, and were conducted sequentially. At the conclusion of each experiment the agent performance during and after training was analysed to provide motivation for subsequent experiments.

Experiments used identical actor and critic neural network architectures. Actor network architectures consisted of an input layer (7 perceptrons), a single hidden layer (256 perceptrons), and an output layer (2 perceptrons). Hidden layers used ReLU activation functions, and the output layer used a $\tanh$ activation function. Critic network architectures consisted of an input layer (7 perceptrons), three hidden layers (256, 256, and 128 perceptrons), and an output layer (1 nodes). Hidden layers used LReLU activation functions, and the output layer used no activation function. Note that the network architectures were similar to those outlined in Table \ref{tab:4102} except that a total of 7 input features were used. Inputs were sourced from differential equation outputs for the two area power system model. The additional inputs were used in an attempt to provide neural networks with more information and reduce the number of episodes required for training. Hyperparameters were held constant for each experiment --- details are shown in Table \ref{tab:5000_hyperparameters}.

\begin{table}[h]
	\centering
	\caption{DDPG hyperparameters used for preliminary investigation experiments.}
	\begin{tabular}{lrlr}
	\toprule
	\textbf{Hyperparameter} & \textbf{Value} & \textbf{Hyperparameter} & \textbf{Value} \\
	\midrule
	Buffer Size 	 & $1 \times 10^6$  & Batch Size 	& 256 \\
	Gamma ($\gamma$) & 0.99 	& Tau ($\tau$) 	& $1 \times 10^{-3}$ \\
	Actor Learning Rate ($\alpha_{\texttt{actor}}$) & $1 \times 10^{-4}$ & Critic Learning Rate ($\alpha_{\texttt{critic}}$) & $3 \times 10^{-4} $ \\
	Weight Decay & 0.00 & & \\
	\bottomrule
	\end{tabular}\label{tab:5000_hyperparameters}
\end{table}

\newpage

Configuration of termination conditions, reward functions, and OU noise noise parameters for experiments A through F can be found in Tables \ref{tab:5001} through \ref{tab:5006}, respectively. The tables also contain the cumulative reward for a DDPG agent trained for 200 episodes under the given settings, for a single episode. The reward for an optimal PI controller under the same reward function settings is provided for comparison. 

Experiments A and B implemented a condition whereby the episode was terminated early if a frequency change in either area exceeded 0.75 Hz. Given this additional termination condition, the agent received a positive reward for each time step it remained in the allowable region. This positive reward was reduced by frequency deviations, tieline power deviations, and control signal magnitudes. Settings for the OU noise parameters were identical to experiments undertaken by Lillicrap \textit{et alias}.

Motivated by the poor agent performance in eperiments A and B, experiments C, D, and E removed the early termination condition and allowed the agent to explore the full state-action space for the entire 30 sec duration. Under this scenario the agent received a negative reward based on the magnitude of frequency deviations, tieline power deviations, and control signals.

The final experiment was motivated by an observation of overly noisy control signals during episodic training for experiments A through E. Experiment F maintained the termination condition and reward structure of experiments C, E, and R; however, modified the OU noise parameters. Parameter magnitudes were reduced by a factor of 10.

A series of plots were created for each experiment A through F, which are shown in Figures \ref{} through \ref{}, respectively. Subplot (a) of Figure \ref{fig:results_A}, and the average of these cumulative rewards are shown in part (b). The DDPG controller performance is shown in Figure \ref{fig:results_A} parts (c) and (d). The 


%--------------------- S: Prelim. Exp. A Setup

\input{./chapters/chap5_exp_a_1}

%--------------------- S: Prelim. Exp. B Setup

\input{./chapters/chap5_exp_b_1}

%--------------------- S: Prelim. Exp. C Setup

\input{./chapters/chap5_exp_c_1}

%--------------------- S: Prelim. Exp. D Setup

\input{./chapters/chap5_exp_d_1}

%--------------------- S: Prelim. Exp. E Setup

\input{./chapters/chap5_exp_e_1}

%--------------------- S: Prelim. Exp. F Setup

\input{./chapters/chap5_exp_f_1}

\clearpage

%--------------------- S: Prelim. Exp. A Results

\input{./chapters/chap5_exp_a_2}

%--------------------- S: Prelim. Exp. B Results

\input{./chapters/chap5_exp_b_2}

%--------------------- S: Prelim. Exp. C Results

\input{./chapters/chap5_exp_c_2}

%--------------------- S: Prelim. Exp. D Results

\input{./chapters/chap5_exp_d_2}

%--------------------- S: Prelim. Exp. E Results

\input{./chapters/chap5_exp_e_2}

%--------------------- S: Prelim. Exp. F Results

\input{./chapters/chap5_exp_f_2}
