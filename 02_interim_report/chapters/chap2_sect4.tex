\section{Deep Reinforcement Learning}\label{sec:deep_reinforcement_learning}
Deep reinforcement learning (DRL) refers to the use of deep neural networks in reinforcement learning frameworks. In this context, neural networks have been used successfully with the Q-learning algorithm as state-action value function approximators. Additionally, they have also seen success when used as function approximators for the policy directly. This section discusses two of the most prominent DRL algorithms in use today: Deep Q-learning, and the Deep Deterministic Policy Gradient.

\subsection{Deep Q-Learning}\label{ssec:deep_q_learning}
Reinforcement learning algorithms outlined in \textsection \ref{sec:reinforcement_learning} are capable of controlling environments with low dimensional state space representations, but perform poorly in environments with high dimensional state space representations. In 2015 Mnih \textit{et alias} published an influential paper that presented a novel agent: a Deep Q-network (DQN) \cite{Mnih2015}. The agent was able to take advantage of recent developments in the field of deep neural networks, discussed in \textsection \ref{sec:deep_neural_networks}, and combine them with the Q-learning algorithm, discussed in \textsection \ref{sec:reinforcement_learning}. By adapting the Q-learning algorithm, Mnih \textit{et al.} trained a neural network, $Q(s,a|\theta)$, to act as a function approximator for the state-action value function $Q(s,a)$. This approach was able to train agents that achieved human levels of performance in video games.

Historically, the applications of neural networks to reinforcement learning have performed poorly because they were unstable and divergent. One of the reasons for this is because of temporally correlated sequences of states, actions, and rewards arising during an episode causing over fitting in the network during training \cite{Tsitsiklis1997}. Mnih \textit{et al.} overcame the temporal correlations problem by creating a buffer, $D$, to store agent experience, $(s_t, a_t, r_t, s_{t+1})$. At each time step, experience is recalled from the buffer via uniform random sampling. The samples are then used to train the network. The technique is referred to as \textit{experience replay}.

The other main innovation from DQN was the use of a second neural network called the \textit{target network}, represented by $Q(s,a | \theta^-)$. The target network parameters, $\theta^-$, are a copy of the original network parameters $\theta$, that are updated periodically. Letting the notation $(s,a,r,s^\prime) \sim U(D)$ denote uniform random sampling from the buffer, the DQN loss function can be expressed as the mean-squared error of the Q-learning (\ref{eq:214}) temporal difference update:
\begin{equation}
	L_i(\theta_i) = \mathbb{E}_{(s,a,r,s^\prime) \sim U(D)} \bigg[ \bigg( r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime | \theta_i^-) - Q(s, a | \theta_i) \bigg)^2 \bigg] \label{eq:2401}
\end{equation}

Applying gradient descent using backpropagation to Equation \ref{eq:2401} adjusts network weights, as described in \textsection \ref{sec:networktraining}. If the target network is not used then a correlation arises between the state-action values $Q(s,a | \theta)$ and the target values, $r + \gamma \max_{a^\prime} Q(s^\prime,a^\prime | \theta)$, causing unstable learning, similar to the temporal correlation problem described above.

The DQN algorithm is summarised in listing \ref{alg:2401}.

\begin{algorithm}[h]
	\caption{DQN Algorithm}
	\begin{algorithmic}[1]
		\State{Initialise replay memory $D$ to capacity $N$}
		\State{Initialise action-value function $Q$ with random weights $\theta$}
		\State{Initialise target action-value function $Q$ with weights $\theta^- = \theta$}
		\For{$episode \gets 1:M$}
			\State{Receive initial observation state $s_1$}
			\For{$t \gets 1:T$}
				\parState{With probability $\epsilon$ select a random action $a_t$ otherwise select \\ $a_t = \arg\max_a Q(s_t, a; \theta)$}
				\State{Execute action $a_t$ and observe reward $r_t$ and state $s_{t+1}$}
				\State{Store transition ($s_t$, $a_t$, $r_t$, $s_{t+1}$) in $D$}
				\State{Sample random minibatch of transitions ($s_j$, $a_j$, $r_j$, $s_{j+1}$) from $D$}
				\State{Set $y_j = \begin{cases}
									r_j \ \ \text{if episode terminates at step} \ \ j+1 \\
									r_j + \gamma \max_{a^\prime} Q(s_{j+1}, a^\prime | \theta^-) \ \ \text{otherwise}
								  \end{cases}$}
				\parState{Perform a gradient descent step on $\big( y_j - Q(s_j, a_j | \theta) \big)^2$ with respect to network parameters $\theta$}
				\State{Every $C$ steps reset $\theta^- = \theta$}
			\EndFor
		\EndFor
	\end{algorithmic}\label{alg:2401}
\end{algorithm}

The DQN algorithm is able to train neural networks to solve problems with high-dimensional state spaces; however, it is only suitable for discrete, low-dimensional actions spaces (REFERENCE). This is because the algorithm needs to calculate the maximum over actions during the update step, as shown in line 11 of Listing \ref{alg:2401}. The implication being that each update step would be computationally expensive for action spaces with a large number of discrete actions, and computationally intractable for continuous action spaces (REFERENCE).

\subsection{Deep Deterministic Policy Gradient}\label{ssec:deep_deterministic_policy_gradient}
In late 2015 Lillicrap \textit{et al.} adapted key ideas from DQN and the deterministic policy gradient theorem from \cite{Silver2014} and developed the first algorithm that could train a neural network successfully on problems with continuous action domains \cite{Lillicrap2015}. The algorithm is called Deep Deterministic Policy Gradient (DDPG), and uses a total of four neural networks. The first network is called the actor, $\pi(s|\theta^{\pi})$, where $\theta^{\pi}$ denotes the network parameters. The actor part of the DDPG agent is classified as a policy search method, discussed in \textsection \ref{sec:policy_search_method}. The second network is called the critic, $Q(s,a|\theta^{Q})$, where $\theta^Q$ denotes the network parameters. The critic part of the DDPG agent is classified as a value function method, discussed in \textsection \ref{sec:value_function_method}.

DDPG makes use of DQN's target network idea, implementing a further two neural networks --- one for each of the actor and critic networks. The network parameters for the actor and critic target networks are denoted $\theta^{\pi^\prime}$ and $\theta^{Q^\prime}$, respectively. DDPG also makes use DQN's experience replay buffer to store experience which is randomly sampled from during training.

The loss function for the critic network is similar to the DQN loss function (\ref{eq:2401}) except that actions are selected by the actor network. Using the standard Q-learning update and the mean square error, the critic loss function is expressed as:
\begin{equation}
	L_i(\theta^Q_i) = \mathbb{E}_{(s,a,r,s^\prime) \sim U(D)} \bigg[ \bigg( r + \gamma Q \big(s^\prime, \pi\big(s^\prime | \theta^{\pi^\prime}_i \big) | \theta^{Q^\prime}_i \big) - Q \big(s,a | \theta^Q_i \big) \bigg)^2 \bigg]
\end{equation}\label{eq:2402}

The actor network is updated using the deterministic policy gradient theorem \cite{Silver2014}. The gradient update is given by:
\begin{equation}
	\nabla_{\theta^\pi} J(\theta^\pi) = \mathbb{E}\big[ \nabla_a Q(s,a | \theta^Q)|_{s, \ a=\pi(s|\theta^\pi)} \nabla_{\theta^\pi} \pi(s|\theta^\pi) \ | \ s \big]
\end{equation}\label{eq:2405}



\begin{algorithm}[h]
	\caption{DDPG Algorithm}
	\label{alg:ddpg}
	\begin{algorithmic}[1]
		\State{Randomly initialise critic $Q(s,a|\theta^Q)$ and actor $\mu(s|\theta^Q)$ with weights $\theta^Q$ and $\theta^\mu$}
		\State{Initialise target networks $Q\prime$ and $\mu\prime$ with weights $\theta^{Q^\prime} \leftarrow \theta^Q$, $\theta^{\mu^\prime} \leftarrow \theta^\mu$}
		\State{Initialise replay buffer $R$}
		\For{$episode \gets 1:M$}
			\State{Initialise a random process $\mathcal{N}$ for action exploration}
			\State{Receive initial observation state $s_1$}
			\For{$t \gets 0:(T-1)$}
				\State{Select action $a_t = \mu(s_t|\theta^\mu) + \mathcal{N}_t$ with noise exploration noise}
				\State{Execute action $a_t$ and observe reward $r_t$ and observe new state $s_{t+1}$}
				\State{Store transition ($s_t$,$a_t$,$r_t$,$s_{t+1}$) in $R$}
				\State{Sample a random minibatch of $N$ transitions ($s_i$,$a_i$,$r_i$,$s_{i+1}$) from $R$}
				\State{Set $y_i = r_i + \gamma Q'(s_{i+1}, \mu'(s_{i+1}|\theta^{Q'}))$}
				\State{Update critic by minimising the loss: $L = \frac{1}{N} \sum_{i}(y_i - Q(s_i, a_i|\theta^Q))^2$}
				\State{Update the actor policy using the sampled policy gradient:
				\begin{equation*}
					\nabla_{\theta^\mu}J \approx \frac{1}{N} \sum_{i} \nabla_a Q(s,a|\theta^Q)|_{s = s_i, a = \mu(s_i)} \nabla_{\theta^\mu}\mu(s|\theta^\mu)|_{s = s_i}
				\end{equation*}
				}
				\State{Update the target networks:
				\begin{align*}
					\theta^{Q'} &= \tau \theta^Q + (1 - \tau) \theta^{Q'} \\
					\theta^{\mu'} &= \tau \theta^{\mu} + (1 - \tau) \theta^{\mu'}
				\end{align*}
				}
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}