\section{Preliminary Investigation}\label{sec:preliminary_investigation}
As discussed in \textsection XXXX, DDPG performance has been observed to suffer from instability and variance,  affecting the agent's ability to learn. Some of the main causes of poor agent performance are: poorly specified reward functions, 

basically need to talk about how a two area power system model was found with a tuned PI controller - the parameters need to be specified and then systematic tests training the ddpg controller can be undertaken and compared against the pi controller.

reward function, network selection, hyperparameters, control signal clipping,

need to ensure that the pi controller is tuned



%---------------- SS: Reward Function

\subsection{Reward Function}

%---------------- SS: DDPG Neural Network Selection

\subsection{DDPG Neural Network Selection}

%---------------- SS: Hyperparams

\subsection{Training Hyperparameters}
Tuned hyperparameters play a large role in eliciting the best results from DDPG

%---------------- SS: OU Process

\subsection{Ornsteinâ€“Uhlenbeck Process}

