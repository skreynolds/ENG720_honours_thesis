\section{Simulation Experiments}\label{sec:preliminary_investigation}
\subsection{Overview}
As discussed in \textsection \ref{ssec:deep_deterministic_policy_gradient}, DDPG performance has been observed to suffer from instability and variance during training. This can impact the agent's ability to learn useful control policies. Some of the main causes of variability in agent performance are due to neural network architecture, choice of activation function, exploratory noise processes, agent experience quality, and agent experience variability. The simulation experiment aims are twofold. Firstly, identify parameter settings and network architectures for which a DDPG agent can learn a frequency control policy comparable to an optimally tuned proportional-integral (PI) controller for a two area power system. Secondly, hyperparameter settings will be identified to ensure agent learning is stable and fast.

To achieve these aims, a series of experiments will be undertaken in which a neural network will be trained using DDPG to eliminate frequency change in a two area power system subjected to load demand changes. Each experiment will be run for an identical fixed number of training episodes, with each episode lasting for a 30 second duration. Each experiment will modify the neural network architecture or the DDPG algorithm to find the most effective control policy that demonstrates stable and fast learning. Fixed random seeds will be used to ensure each experiment sees identical training scenarios, and to ensure results can be replicated. Table \ref{tab:experiment_overview} provides an overview of proposed experiments. Agent performance will be evaluated according to the criteria outlined in section \ref{sec:agent_performance}.
\begin{table}[h]
	\centering
	\caption{An overview of the experiments that will undertaken to achieve the best neural network performance for load frequency control of a two area power system.}\label{tab:experiment_overview}
	\begin{tabular}{lp{9cm}}
		\toprule
		\textbf{Name} & \textbf{Description} \\
		\midrule
		Baseline & An identical neural network architecture used in Lillicrap \textit{et alias}, with hidden layers of (400, 300) will be used to form a basis comparisons against subsequent experiments.\\
		 & \\
		Neural Network Architecture & A neural network architecture that has smaller number of nodes in hidden layers (64, 64) will be used to determine if a smaller network is more effective. \\
		 & \\
		Activation Functions & LReLU activation functions will be used for neural network hidden layers to determine if they are more effective.\\
		 & \\
		OU Noise Process & Exploratory OU noise process parameters, $\sigma$ and $\theta$, will be modified to determine the impact on agent learning. \\
		 & \\
		Prioritised Experience Replay & Priority metric added to experience replay buffer allowing useful experience  to be visited more often.\\
		 & \\
		Expert Learner & PID controller experience (the expert) will be introduced to the experience replay buffer with half of all experience from PID interaction with the environment.\\
		 & \\
		Stochastic Demand & A stochastic load demand profile will be used to perturb the system to provide the agent with more variability in training scenarios.\\
		\bottomrule
	\end{tabular}
\end{table}




\subsection{Measuring agent performance}\label{sec:agent_performance}
MAKE SURE TO PROVIDE CLEAR METHODOLOGY ON HOW THE THE AGENTS TRAINING PROGRESS WILL BE MEASURED AND COMPARED

MAKES SURE THAT IT IS CLEAR ON HOW THE CONTROLLER PERFORMANCE WILL BE MEASURED AND THE COMPARISONS THAT WILL BE MADE FOR EACH OF THE EXPERIMENTS.