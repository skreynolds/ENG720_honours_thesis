\section{Simulation Experiments}\label{sec:simulation_experiments}
\subsection{Overview}
As discussed in \textsection \ref{ssec:deep_deterministic_policy_gradient}, DDPG performance has been observed to suffer from instability and variance during training. This can impact the agent's ability to learn useful control policies. Some of the main causes of variability in agent performance are due to neural network architecture, choice of activation function, exploratory noise processes, agent experience quality, and agent experience variability.

In response to this, the simulation experiment aims are therefore as follows:
\begin{itemize}
	\item Identify parameter settings, network architectures, and network training conditions for which a DDPG agent can learn a frequency control policy comparable to an optimally tuned proportional-integral (PI) controller for a two area power system.
	\item Determine training variables and conditions to ensure agent learning is stable and fast.
\end{itemize}

To achieve these aims, a series of experiments shall be undertaken in which a neural network is trained using DDPG to perform the task of load frequency control in a two area power system subjected to load demand changes. Experiments will modify a single variable in either the neural network architecture or the DDPG algorithm, holding all else constant. Each experiment will consist of a training phase and a performance evaluation phase. The training phase, outlined in \textsection \ref{ssec:training}, will see neural network weights modified using experience collected from the agent's interaction with the environment. The performance evaluation phase, outlined in \textsection \ref{ssec:testing}, will compare the trained agent performance against an optimally tune PI controller for the same task. Agent performance metrics are presented in \textsection \ref{sec:agent_performance}, and detailed descriptions of each experiment are provided in \textsection \ref{sec:baseline} to \ref{sec:stochastic}.

\subsection{Training}\label{ssec:training}
At the beginning of each experiment a new instance of a DDPG agent shall be initialised, and the DDPG agent replay buffers cleared. Each experiment shall use the same episode scenario, with the system and agent response simulated for a total of 30 sec, after which the episode will terminate. In order to simulate a power system perturbation, a $\pm$0.01pu step change in the power demand for Area 1 will be introduced at a random time between the 0 and 30 sec mark. An example of a +0.01pu step change occurring at the 15 sec mark is shown in Figure \ref{fig:5001_demand_profile}. Note that this perturbation type was not used for the final experiment, where a stochastic perturbation signal was used instead.

\begin{figure}[h]
	\centering
	\input{./figures/5001_demand_profile/demand_profile.tikz}
	\caption[Preliminary investigation load demand step change]{At the 15 sec mark the system experiences a step load change in the power demand in Area 1, and the simulation continues for 30 sec thereafter.}
	\label{fig:5001_demand_profile}
\end{figure}

From initialisation, the simulation shall be incrementally stepped forward by 0.01 sec for a total of 3000 steps. At each time step the DDPG agent shall be trained using experience stored in the replay buffer from current and previous system interactions, for a given experiment. Agent training shall be run for a total of 10000 episodes for each experiment.

DDPG training algorithm hyperparameters shall be held constant for each experiment, using the same values from the experiments conducted by Lillicrap \textit{et alias} \cite{Lillicrap2015}. Hyperparameter values used for the experiments described in this chapter are documented in Table \ref{tab:5000_hyperparameters}.

\begin{table}[h]
	\centering
	\caption{DDPG hyperparameters used for preliminary investigation experiments.}
	\begin{tabular}{lrlr}
	\toprule
	\textbf{Hyperparameter} 							& \textbf{Value} 		& \textbf{Hyperparameter} 								& \textbf{Value} 		\\
	\midrule
	Buffer Size 	 									& $1 \times 10^6$  		& Tau ($\tau$) 						  					& $1 \times 10^{-3}$ 	\\
	Gamma ($\gamma$) 									& 0.99 					& Critic Learning Rate ($\alpha_{\texttt{critic}}$) 	& $3 \times 10^{-4} $   \\
	Actor Learning Rate ($\alpha_{\texttt{actor}}$) 	& $1 \times 10^{-4}$ 	& OU Noise $\mu$ 										& 0.00					\\
	Weight Decay 										& 0.00 					& OU Noise $\theta$										& 0.15					\\
	Batch Size 											& 256 					& OU Noise $\sigma$										& 0.20					\\
	\bottomrule
	\end{tabular}\label{tab:5000_hyperparameters}
\end{table}

Each experiment, detailed in the remaining sections of this chapter, shall modify a single variable with respect to the agent or training algorithm construction, while holding other variables constant. Training performance shall be captured after each episode. The performance metrics that will be used to measure training performance are detailed in \textsection \ref{sec:agent_performance}. Plots shall be created visually demonstrate agent learning performance throughout the 10000 episode training duration. Details of the plots are outlined in \textsection \ref{sec:agent_performance}.

\subsection{Agent performance evaluation}\label{ssec:testing}
At the conclusion of training, trained neural network model weights shall be reloaded into a new instance of the neural network architecture to evaluate agent performance. Ten independent tests shall be carried out on each agent. A single test shall simulate the two area power system for a total of 30 sec, during which time Area 1 shall be perturbed using a step change of $\pm$0.01pu. Perturbations shall be introduced at different times. The first 5 tests will see +0.01pu perturbations at the 5 sec, 10 sec, 15 sec, 20 sec, and 25 sec marks, respectively. The final 5 tests will see $-$0.01pu perturbations introduced at the 5 sec, 10 sec, 15 sec, 20 sec, and 25 sec marks, respectively.

Timeseries data of frequency and control actions shall be captured for each test. The data will be used to calculate a series of metrics to measure the trained agent performance. The timeseries data shall also be used to develop a series of plots to visually represent agent performance. Details of metrics and plots for agent performance evaluation are provided in \ref{sec:agent_performance}.


\subsection{Performance metrics}\label{sec:agent_performance}
\subsubsection{Training performance metrics}
A reward signal, $r_t$, shall be calculated for the agent at each timestep. Summing the reward from each of the 3000 timesteps during an episode provides a cumulative reward, $G_{3000}$. Whilst this metric could be used to report agent training performance, high variability of raw cumulative reward signals mean this metric is not often used \cite{Henderson2017}. Instead, agent training performance shall be assessed using a moving average based on cumulative rewards for 100 episodes.

This metric will be referred to as MACR100 for the remainder of this thesis, and is described mathematically as:
\begin{equation}
	(MACR100)_{\texttt{episode}} = \frac{1}{100} \sum_{n = 0}^{99} (G_{3000})_{\texttt{episode} - n}
\end{equation}

A plot of the MACR100 shall be developed for each experiment showing the evolution of this metric over the 10000 episodes during agent training.

\subsubsection{Agent performance evaluation metrics}
Raw cumulative reward shall be reported for each independent trial during agent performance evaluation, which will be used as the main metric of comparison between neural networks and optimally tuned PI controllers.

The following additional metrics shall also be reported for each power area using frequency and control action time series data captured during each independent trail. The metrics shall be calculated for each power area, and are as follows:
\begin{itemize}
	\item \textbf{Maximum frequency deviation}: the maximum absolute frequency deviation during the 30 sec simulation period. Mathematically this is expressed as:\\
		\begin{equation}
			f_{max} = \max \big\{ f_1, f_2, \dotsc, f_{3000} \big\}
		\end{equation}
	
	\item \textbf{Average frequency deviation}: the average absolute frequency deviation during the 30 sec simulation period. The metric is calculated as follows:\\
		\begin{equation}
			f_{average} = \frac{1}{3000}\sum_{n=1}^{3000} f_n
		\end{equation}
		
	\item \textbf{Maximum control effort}: the maximum absolute control effort during the 30 sec simulation period. The metric is calculated as follows:\\
		\begin{equation}
			u_{max} = \max \big\{ u_1, u_2, \dotsc, u_{3000} \big\}
		\end{equation}
		
	\item \textbf{Average control effort}: the average control effort during the 30 sec simulation period. The metric is calculated as follows:\\
		\begin{equation}
			u_{average} = \frac{1}{3000}\sum_{n=1}^{3000} u_n
		\end{equation}
		
	\item \textbf{Settling time}: the number of seconds the frequency signal took to fall within 0.001 of the final value. Note that the final signal value is not necessarily zero.
\end{itemize}

The following plots shall be created for each independent trial during agent performance evaluation:

\begin{itemize}
	\item \textbf{Frequency:} a timeseries plot showing an evolution of power system frequency under neural network control, for a given power system area, over the 30 sec simulation. Two plots will be created for each experiment --- one for each power system area. Note that system frequency under optimal PI control will also be included for comparison.
	\item \textbf{Control signal:} a timeseries plot showing an evolution of the control action issued by the neural network over the 30 sec simulation. Two plots will be created for each experiment --- one for each experiment. Note that control signals from an optimal PI controller will also be included for comparison.
\end{itemize}

\subsection{Baseline Experiment Details}\label{sec:baseline}
An identical neural network architecture to that used in Lillicrap \textit{et alias} will be used for the baseline experiment \cite{Lillicrap2015}. The network shall makes use of ReLU activation functions, and be comprised of two hidden layers. The first hidden layer shall have 400 perceptrons, and the second hidden layer 300 perceptrons. The proposed neural network architecture for actor and critic networks to be used in this experiment is detailed in Table \ref{tab:4101}.

\begin{table}[h]
	\centering
	\caption{An overview of actor and critic neural network architectures used in experiments undertaken by Lillicrap \textit{et alias}}
	\begin{tabular}{@{\extracolsep{6pt}}llrlr@{}}
		\toprule
		 & \multicolumn{2}{c}{\textbf{Actor}} & \multicolumn{2}{c}{\textbf{Critic}} \\ 
		\cline{2-3} \cline{4-5}
		\multirow{2}{*}{\textbf{Layer}} & \textbf{Activation} & \textbf{Number of} & \textbf{Activation} & \textbf{Number of} \\
		 &  \textbf{Function} & \textbf{Perceptrons} & \textbf{Function} & \textbf{Perceptrons} \\
		\midrule
		Input Layer & None & 7 & None & 7 \\
		Hidden Layer 1 & ReLU & 400 & ReLU & 400 \\
		Hidden Layer 2 & ReLU & 300 & ReLU & 300 \\
		Output Layer & $\tanh$ & 2 & None & 2 \\
		\bottomrule
	\end{tabular}
	\label{tab:4101}
\end{table}

\subsection{Neural Network Experiment Details}
Islam \textit{et alias} has shown that policy network architecture selection can significantly impact results for DDPG. Specifically, the policy network architecture can impact the maximum reward that the agent can achieve \cite{Islam2017}. As such, this experiment shall examine a smaller network architecture commonly seen in the literature \cite{Henderson2017, Islam2017}. The network architecture shall consist of two hidden layers, both with 64 perceptrons. The network will retains the use of ReLU activation functions. A proposed neural network architecture for actor and critic networks to be used in this experiment is detailed in Table \ref{tab:4102}.

\begin{table}[h]
	\centering
	\caption{An overview of actor and critic neural network architectures used in the neural network architecture experiment.}
	\begin{tabular}{@{\extracolsep{6pt}}llrlr@{}}
		\toprule
		 & \multicolumn{2}{c}{\textbf{Actor}} & \multicolumn{2}{c}{\textbf{Critic}} \\ 
		\cline{2-3} \cline{4-5}
		\multirow{2}{*}{\textbf{Layer}} & \textbf{Activation} & \textbf{Number of} & \textbf{Activation} & \textbf{Number of} \\
		 &  \textbf{Function} & \textbf{Perceptrons} & \textbf{Function} & \textbf{Perceptrons} \\
		\midrule
		Input Layer & None & 7 & None & 7 \\
		Hidden Layer 1 & ReLU & 64 & ReLU & 64 \\
		Hidden Layer 2 & ReLU & 64 & ReLU & 64 \\
		Output Layer & $\tanh$ & 2 & None & 2 \\
		\bottomrule
	\end{tabular}
	\label{tab:4102}
\end{table}

\subsection{Activation Function Experiment Details}
Certain activation functions, such as ReLU, have been shown to cause worsened learning performance due to the dying relu problem \cite{Xu2015}, outlined in \textsection \ref{sec:activation_functions}. As such, this experiment examines the use of LReLU activation functions. An identical neural network architecture to that used in the baseline experiment shall be used; however, LReLU activation functions will be implemented on neural network hidden layers. The proposed architecture is motivated by research undertaken by Henderson \textit{et alias} \cite{Henderson2017}, and is detailed in Table \ref{tab:activation_function_experiment_architecture}.

\begin{table}[h]
	\centering
	\caption{An overview of actor and critic neural network architectures used for the activation function experiment.}\label{tab:activation_function_experiment_architecture}
	\begin{tabular}{@{\extracolsep{6pt}}llrlr@{}}
		\toprule
		 & \multicolumn{2}{c}{\textbf{Actor}} & \multicolumn{2}{c}{\textbf{Critic}} \\ 
		\cline{2-3} \cline{4-5}
		\multirow{2}{*}{\textbf{Layer}} & \textbf{Activation} & \textbf{Number of} & \textbf{Activation} & \textbf{Number of} \\
		 &  \textbf{Function} & \textbf{Perceptrons} & \textbf{Function} & \textbf{Perceptrons} \\
		\midrule
		Input Layer & None & 7 & None & 7 \\
		Hidden Layer 1 & LReLU & 400 & LReLU & 400 \\
		Hidden Layer 2 & LReLU & 300 & LReLU & 300 \\
		Output Layer & $\tanh$ & 2 & None & 2 \\
		\bottomrule
	\end{tabular}
	\label{tab:4101}
\end{table}

\subsection{OU Noise Hyperparameter Experiment Details}
Lillicrap \textit{et alias} used OU noise to ensure continuous action space exploration during training \cite{Lillicrap2015}. Typically, the noise process is decayed after each episode by some arbitrary amount, so that as agent training progresses there is less state-action space exploration and more policy exploitation. The benefits of state-action space exploration versus policy exploitation were prebiously discussed in \textsection \ref{sec:value_function_method}. The principal motivation for this experiment is to increase the amount of state-action space exploration to achieve better policy outcomes.

For the OU noise experiment, neural network architectures and hyperparameters shall be identical to the baseline experiment. Moreover, the OU noise process parameters shall be identical to the parameters detailed in Table \ref{tab:5000_hyperparameters}. In all other experiments presented in this thesis, decay is applied to the OU noise process by multiplying OU noise parameters by a decay factor of 0.99 after each episode. This experiment shall use a 0.999 decay multiplier, resulting in the exploratory noise superimposed on control actions for longer.

\subsection{Prioritised Experience Replay Experiment Details}
Prioritised experience replay, originally developed by Schaul \textit{et alias} in 2015, is a modification to deep reinforcement learning architectures that allows an agent to place importance on experiences stored in the experience replay buffer \cite{Schaul2015}. Typically, experience is sampled uniformly during training; however, under a prioritised experience replay framework, experience with a higher importance is sampled more often. The idealised criterion on which importance is assigned to an experience is the amount the agent can learn from a transition from one state to another. While this is not directly accessible, Schaul \textit{et alias} proposed that a reasonable estimate is provided by the temporal difference error, which is an artefact of Q-learning presented in Algorithm \ref{alg:02_q_learning}.

Since DDPG makes use of termporal difference methods, the prioritised experience replay experiment shall implement a prioritised experience replay framework. The motivation is that it will take less time for the agent to converge on a useful policy, meaning that subsequent stages of training should yield better policies.

\subsection{Expert Learner Experiment Details}
A paper by Yan, published in late June 2020, demonstrated it was possible to develop a neural network for load frequency control of a two area power system. The neural network was trained using DDPG; however, the application of the DDPG algorithm was only to provide minor modification to pre-trained model weights. Model pre-trained consisted of supervised learning using a dataset obtained from an optimally tuned PI controller for the load frequency task \cite{Yan2020}. Yan noted further that without the use of pre-training the model using PI controller data randomly initialised DNNs may take a long time to converge on an optimal policy, or not converge at all.

The expert learner experiment shall use a PI controller, optimised for the load frequency control task, to provide half of all experience in the memory buffer. This is motivated by the pre-training used by Yan, that saw neural network model weights pre-trained using the experience of a optimally tuned PI controller.

\subsection{Stochastic Demand Experiment Details}\label{sec:stochastic}
Statistical methods used to predict continuous variables, such as ordinary least squares for regression, see improved performance when the datasets used for analysis contain sufficient variance across explanatory variable domains. This experiment is motivated by the idea that better agent performance may be achieved from load demand perturbations with greater variance.

All other proposed experiments have made use of $\pm$0.01 step change disturbances at a random time between the 0 and 30 sec mark. This experiment shall use a stochastic load demand profile. The stochastic load demand profile will be initialised at 0.00pu at the start of an episode. At each time step, the load demand profile shall increment by random amount selected from the interval [-0.001, 0.001]. Values taken by the stochastic load demand profile shall be capped at $\pm$0.01.

An example of the proposed stochastic load demand profile used for a single 30 sec episode is demonstrated in Figure \ref{fig:stochastic_signal}.

\begin{figure}[h]
	\centering
	\input{./figures/4201_stochastic_demand_profile/stochastic_demand_profile.tikz}
	\caption{An example of a stochastic load demand profile introduced in Area 1 for the stochastic load demand experiment.}\label{fig:stochastic_signal}
\end{figure}

