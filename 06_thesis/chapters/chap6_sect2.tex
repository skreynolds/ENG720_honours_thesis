\section{Baseline Experiment}

Actor network architectures consisted of an input layer (7 perceptrons), a single hidden layer (256 perceptrons), and an output layer (2 perceptrons). Hidden layers used ReLU activation functions, and the output layer used a $\tanh$ activation function. The network architectures of the critic consisted of an input layer (7 perceptrons), three hidden layers (256, 256, and 128 perceptrons), and an output layer (1 nodes). Hidden layers used LReLU activation functions, and the output layer used no activation function.

\begin{table}[h]
	\centering
	\caption{An overview of actor and critic neural network architectures used in experiments undertaken by Lillicrap \textit{et alias}}
	\begin{tabular}{@{\extracolsep{6pt}}llrlr@{}}
		\toprule
		 & \multicolumn{2}{c}{\textbf{Actor}} & \multicolumn{2}{c}{\textbf{Critic}} \\ 
		\cline{2-3} \cline{4-5}
		\multirow{2}{*}{\textbf{Layer}} & \textbf{Activation} & \textbf{Number of} & \textbf{Activation} & \textbf{Number of} \\
		 &  \textbf{Function} & \textbf{Perceptrons} & \textbf{Function} & \textbf{Perceptrons} \\
		\midrule
		Input Layer & None & 3 & None & 3 \\
		Hidden Layer 1 & ReLU & 400 & ReLU & 400 \\
		Hidden Layer 2 & ReLU & 300 & ReLU & 300 \\
		Output Layer & $\tanh$ & 2 & None & 2 \\
		\bottomrule
	\end{tabular}
	\label{tab:4101}
\end{table}

\begin{figure}[h]
	\centering
	\input{./figures/6201_average_reward_plot/average_reward_plot.tikz}
	\caption{text}
\end{figure}

\begin{figure}[h]
	\centering
	
	\input{./figures/6202_frequency_response_1/frequency_response_1.tikz}
	\caption{text}
	
	\input{./figures/6203_control_signal_1/control_signal_1.tikz}
	\caption{text}
	
	\input{./figures/6204_frequency_response_2/frequency_response_2.tikz}
	\caption{text}
		
	\input{./figures/6205_control_signal_2/control_signal_2.tikz}
	\caption{text}
\end{figure}