\section{Neural Network Controller Model}

Deep deterministic policy gradient (DDPG), described in \textsection \ref{ssec:deep_deterministic_policy_gradient}, is used to train neural networks to perform control actions given a state observation. DDPG uses actor and critic networks, both of which also have target networks. The implication is that a total of four neural networks are required for a single DDPG controller instance. To accommodate this requirement, two Python classes were created: one for the actor called \verb|Actor|, and another for the critic called \verb|Critic|. Implementations for the \verb|Actor| and \verb|Critic| classes can be found in Appendices \ref{app:implementation_actor_model} and \ref{app:implementation_critic_model}, respectively. Both actor and critic networks were built using an input layer, two hidden layers and an output layer --- an identical structure to experiments conducted by Lillicrap \textit{et al.} \cite{Lillicrap2015}. Neural network hidden layers used rectified linear units (ReLU) for activation functions. The final output layer of the actor network used a $\tanh$ activation function to bound the actions.