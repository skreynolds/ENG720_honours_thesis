\begin{table}[h]
	\centering
	\caption{Experiment A details of environment reward function and termination conditions, DDPG algorithm OU noise settings, and a comparison of trained DDPG agent performance compared to an optimal PI controller.}
	\begin{tabular}{@{\extracolsep{6pt}}cp{2.8cm}p{2.8cm}p{2.8cm}p{2.8cm}}
		\toprule
		\multirow{15}{*}{\rotatebox[origin=c]{90}{\LARGE \textbf{Experiment A}}} & \multicolumn{4}{c}{\textbf{Reward Function}}  \\
		 \rule{0pt}{1.5ex}
		 & \multicolumn{4}{l}{At each time step the agent received a reward of:} \\[0.1cm]
		 & \multicolumn{4}{c}{\small$\begin{aligned}\texttt{r} = 1.2 &- 0.1 \times |\texttt{frequency 1}| - 0.1 \times |\texttt{frequency 2}| \\ &- 0.1 \times |\texttt{tieline}| \\ &- |\texttt{control signal 1}| - |\texttt{control signal 2}|\end{aligned}$}\\
		 & & & & \\
		  & \multicolumn{4}{l}{For early termination the agent received a reward of:} \\[0.1cm]
		  & \multicolumn{4}{c}{$-2050 \times\texttt{(|frequency 1| + |frequency 2|)}$} \\[0.1cm]
		\cline{2-5}\rule{0pt}{2.5ex}
		 & \multicolumn{2}{c}{\textbf{OU Noise}} & \multicolumn{2}{c}{\textbf{Results}}\\
		\cline{2-3}\cline{4-5}\rule{0pt}{2.5ex}
		 & $\mu$ 	& 0.00 & PI   & 3569.22 \\
		 & $\theta$ & 0.15 & DDPG & 2794.51 \\
		 & $\sigma$ & 0.20 & & \\
		 \cline{2-5}\rule{0pt}{2.5ex}  
		 & \multicolumn{4}{c}{\textbf{Termination Condition}}\\
		 & \multicolumn{4}{p{12cm}}{Episode terminated early if the frequency change in either area 1 or area 2 exceeds 0.75, or the episode reached 30 sec}\\
		 \toprule
	\end{tabular}\label{tab:5001}
\end{table}