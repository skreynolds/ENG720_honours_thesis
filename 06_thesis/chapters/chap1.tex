\chapter{Introduction}
In 2018, approximately 261$\si{\tera\watt\hour}$ of power was generated in the Australian electricity sector. Renewables contributed 19\% of the total generation, an increase from 15\% in 2017. The Department of Industry, Science, Energy and Resources have observed an increase in renewable energy generation year-on-year in the electricity generation market since 2008, as shown in Figure \ref{fig:101_renewable_energy} \cite{Diser2020}.\\

\begin{figure}[ht]
	\centering
	\input{./figures/101_renewable_energy/renewable_energy}
	\caption[Renewable power generation over time]{Power generation from renewable sources (dashed line), and total power generation (solid line) in Australia from 1977 to 2018.}
	\label{fig:101_renewable_energy}
\end{figure}

One of the benefits of transitioning from thermal sources of power generation to renewable sources is reduced greenhouse gas emissions \cite{IPCC2012}; however, this transition is not without its drawbacks. With an increased reliance on renewable power generation sources posing challenges for power system stability owing to load management. A recent example is the system failure in Alice Springs, caused by an event cascade that was triggered by cloud cover shadowing a solar array. The system failure left residents in Alice Springs without power for approximately eight hours \cite{UCNT2019}. An independent investigation highlighted that poor control policies were one of the factors that contributed to the blackout. In this instance, the generator provisioned to ramp up in the event of cloud cover was unable to be controlled. Moreover, generators that were still under the control regime were issued operating set points above their rated capacity, which resulted in thermal overload and subsequent tripping from the protection system \cite{Wilkey2019}.

Current control methods use classical feedback loop techniques. These methods can be brittle when faced with system changes, or scenarios which they were not designed for. An improved controller would be one that can learn and adapt its control protocols to an unseen system or event, given some broad control objective. This research proposes a deep reinforcement learning (DRL) agent for controlling the frequency of a power system consisting of multiple generators, and multiple load demands with individual stochastic profiles.

\section{Research Aim}
The principle aim of this research is to compare the performance of known, optimised feedback loop controller architectures against a DRL based control system when tasked with performing load following ancillary services with regulating generators under automatic generation control (AGC) for a two-area power system. This research will be undertaken in order to understand the feasibility of using DRL agents for two-area power system management.

\section{Scope}
The proposed research is concerned with the task of load frequency control using deep reinforcement learning and classical control agents. For the purposes of this research, load frequency control is defined as maintaining system frequency within an allowable region of 49.80 to 50.20$\si{\hertz}$ which are considered to be normal power system operating conditions.

The key performance aspects that will assessed are the controller's ability to maintain system frequency to the desired nominal 50$\si{\hertz}$ value.

The research will focus on two area power systems. Each power area will consist of one regulating generator, and one stochastically fluctuating demand profile. The generator model shall be composed of a governor, turbine, and generator load. The research will primarily consider system frequency as the main controller input; however, other system variables may be used as input features under agent training and inference regimes. Comparison of DRL agent performance will be made against theoretical models of classical control architectures. Performance against practical control architectures implemented by PWC (or other utilities) will not be considered. Research will be conducted in a simulated environment. Agent performance on real hardware will not be explored.

\section{Structure of Interim Report}
The remainder of this report is structured as follows:

Chapter 2 introduces the necessary background to understand work presented later in the report. This includes a formal introduction to reinforcement learning, deep neural networks, and deep reinforcement learning, including associated mathematical preliminaries.

Chapter 3 undertakes a literature review exploring different technologies used to address the load frequency control problem. Topics discussed include feedback loops, fuzzy logic, genetic algorithms, and artificial neural networks. The chapter concludes with a review of DRL applications to load frequency control and the motivation for using these controllers.

Chapter 4 outlines the research approach, providing a discussion on model development and implementation, and a framework for an experimental approach for assessing Deep Reinforcement Learning feasibility for training a neural network for the task of load frequency control.

Chapter 5 presents results from the development phase of the research. This includes mathematical model development for simulation environment and controller models. Additionally, the chapter details software implementation for developed models, neural networks, and deep reinforcement learning algorithms.

Chapter 6 presents experimental results from simulations. This includes neural network training, and trained agent performance evaluation.

Chapter 7 provides a discussion on findings from Chapter 5 and evaluates feasibility of the proposed DRL controller. Chapter concludes with recommendations for the continued direction of this work.

Chapter 8 concludes the research by restating the problem definition, outlining the experimental approach, and summarising the research findings. The chapter highlights useful insights and nominates future directions for the application of deep reinforcement learning to the problem of load frequency control.