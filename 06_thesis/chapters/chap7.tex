\chapter{Analysis and Discussion}

This chapter provides analysis for both agent training and trained agent performance phases of the simulation experiments. For agent training, this will include the comparison of final and highest MACR100 scores for the different simulation experiments. Explanations are offered where an agent performance is poor. Similarly, a comparison of trained agent performance from the different experiments is provided, as well as a comparison to an optimally trained PI controller. Explanations are offered for instances where trained agent performance does not exceed the tuned PI controller.

\section{Training}
Monotonically increasing cumulative reward values indicate a continuous average increase in deep reinforcement learning agent performance. This is a desirable feature for cumulative reward plots. Plots of the MACR100 evolution for each experiment are detailed in Figures \ref{fig:6201_average_reward}, \ref{fig:6301_average_reward}, \ref{fig:6401_average_reward}, \ref{fig:6501_average_reward}, \ref{fig:6601_average_reward}, \ref{fig:6701_average_reward}, and \ref{fig:6801_average_reward}.

Generally, it can be observed that MACR100 is positively correlated with the episode number for all experiments. It is noted that the MACR100 signals were not monotonic for any experiment. Moreover, the MACR100 plots feature small to large fluctuations in the signal with increase in episode. This is especially true for the neural network experiment, expert learner experiment, and stochastic load demand experiment.

During the early stages of training all experiments demonstrated a period during which rapid learning took place. This is indicated by a steep rise in the MACR100 score, which typically took place in the first 1000 episodes of training, as shown in Figure \ref{fig:7101_MACR100_first}. In all experiments the agents demonstrated policy convergence, as indicated in reduced fluctuations of the MACR100 score. Convergence typically took place after the episode 7000, as shown in Figure \ref{fig:7102_MACR100_second}.

\clearpage

\begin{figure}[h]
	\centering
	
	\input{./figures/7101_learning_comparison/learning_comparison_plot.tikz}
	\caption[MACR100 comparison for first 5000 episodes]{Comparison of the MACR100 evolution for all experiments during the first 5000 episodes of training.}\label{fig:7101_MACR100_first}
	
	\vspace{0.5cm}
	
	\input{./figures/7102_learning_comparison/learning_comparison_plot.tikz}
	\caption[MACR100 comparison for last 5000 episodes]{Comparison of the MACR100 evolution for all experiments during the final 5000 episodes of training.}\label{fig:7102_MACR100_second}
\end{figure}

\clearpage

The baseline experiment demonstrated the best MACR100 score at the 10000 episode, and one of the highest MACR100 scorces overall. The exception was the expert learner experiment which yielded a marginally higher score than the baseline during training. Figure \ref{fig:7103_macr100_comparison} provides a comparison of the MACR100 for different experiments at episode 10000. Figure \ref{fig:7104_macr100_comparison} provides a comparison of the highest MACR100 achieved during training for each experiment. The principal observation that can be made from these results is that the original DDPG hyperparameters and neural network architecture used by Lillicrap \textit{et alias} are the best suited to the load frequency control task.

\begin{figure}[h]
	\begin{minipage}{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7103_macr100_comparison/macr100_comparison.tikz}}
		\caption{MACR100 at episode 10000.}\label{fig:7103_macr100_comparison}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7104_macr100_comparison/macr100_comparison.tikz}}
		\caption{Best MACR100 achieved during training.}\label{fig:7104_macr100_comparison}
	\end{minipage}
\end{figure}
 
\section{Trained Agent Performance}
The optimally tuned PI controller was able to perform the load frequency control task better than trained agents across all experiments. This was evident for most performance metrics. Despite agent performance being worse than the PI controller, trained agents were still able to arrest frequency deviations, providing an improvement over a system with no controller. The Baseline experiment demonstrated the best results for most performance metrics; however, the Baseline control policy caused system frequency oscillations, ultimately making it unusable. Control policies observed in the OU Noise and Stochastic Load Demand experiment also performed well. These agents exhibited more stable control characteristics when compared to the Baseline control policy; however, persistent frequency deviation offsets rendered them unusable. 

The remainder of this chapter discusses the trained agent performance with respect to each of the performance metrics.

\subsection{Cumulative Reward}
The optimally tuned PI controller received a better cumulative reward when compared to all experiments for both the +0.01pu and -0.01pu step change disturbances at the 5, 10, 15, 20, and 25 second marks. Figures \ref{fig:7201_cum_reward_pos} and \ref{fig:7202_cum_reward_neg} show the trained agent performance for for +0.01pu and -0.01pu step change disturbances at the 15 sec mark, respectively. The PI benchmark is included as a reference. For the +0.01pu step change disturbance the Baseline experiment performed the best with a cumulative reward marginally worse than the PI controller. The next best agent was the OU Noise experiment. For the -0.01pu step change disturbance the Priority Experience Replay experiment performed the best, closely followed by the Baseline. The worst performing agents were from the Activation Function and the Neural Network experiments.

It is clear that the trained agent cumulative reward performance displays inconsistency across positive and negative step change disturbances. This is evident in both the mismatching of cumulative rewards for +0.01pu and -0.01pu step disturbance scenarios, and also the performance ordering mismatch. This is in contrast to the PI controller, which maintains a consistent cumulative reward for the 15 sec mark in both positive and negative step change disturbance scenarios.
\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7201_cum_reward_comparison/cum_reward_comparison.tikz}}
		\caption[Cumulative reward comparison for +0.01pu step change]{Trained agent cumulative reward for a +0.01pu step change disturbance at the 15 sec mark.}\label{fig:7201_cum_reward_pos}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7202_cum_reward_comparison/cum_reward_comparison.tikz}}
		\caption[Cumulative reward comparison for +0.01pu step change]{Trained agent cumulative reward for a -0.01pu step change disturbance at the 15 sec mark.}\label{fig:7202_cum_reward_neg}
	\end{minipage}
\end{figure}

\subsection{Frequency Deviations}
The optimally tuned PI controller demonstrated better performance, maintaining lower average absolute frequency deviations in Areas 1 and 2 when compared to all experiments for both +0.01pu and -0.01pu step change disturbances at the 5, 10, 15, 20, and 25 sec marks. Baseline and OU Noise experiments were able to show smaller maximum frequency deviations when compared to the PI controller in some instances. Figure \ref{fig:7203_max_freq} shows a comparison of the maximum frequency deviation of  the trained agents for a +0.01pu step change disturbance at the 15 sec mark, and Figure \ref{fig:7204_avg_freq} shows a comparison of the average frequency deviation for the same scenario. Both figures include the PI benchmark for comparison. The worst performing experiment for both average and maximum frequency deviation was the Activation Function.

Despite the Baseline experiment having the best recorded performance, visual inspection of the frequency signal for Areas 1 and 2 show undesirable oscillations occurring after the step change disturbance, resulting in an unusable policy for load frequency control. Oscillations such as these would cause unnecessary wear on generation units, and contribute to instability in the power system. Prior to the step change disturbance the controller was able to demonstrate similar levels of frequency stability seen in the PI controller. Frequency control plots for the Baseline experiment are shown in Figures \ref{fig:6202_freq_1} and \ref{fig:6204_freq_2}.

\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7203_frequency_comparison/frequency_comparison.tikz}}
		\caption[Maximum absolute frequency deviation comparison]{Maximum frequency deviation for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}\label{fig:7203_max_freq}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7204_frequency_comparison/frequency_comparison.tikz}}
		\caption[Average absolute frequency deviation comparison]{Average frequency deviation for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}\label{fig:7204_avg_freq}
	\end{minipage}
\end{figure}

Increased frequency signal stability was observed for both OU Noise and Stochastic Load Demand experiments. Similarly to the Baseline controller, the OU Noise experiment maintained stability prior to the step change disturbance, although the Stochastic Load Demand experiment did not. Both agents were observed to dampen signal ringing; however, despite apparent frequency signal convergence, a zero level offset persisted. This implies OU Noise and Stochastic Load Demand agents, in their current state, are incapable of restoring frequency deviations after a disturbance. This makes them unsuitable for use as load frequency controllers. Frequency control plots for the OU Noise experiment are shown in Figures \ref{fig:6502_freq_1} and \ref{fig:6504_freq_2}. Frequency control plots for the Stochastic Load Demand experiment are shown in Figures \ref{fig:6802_freq_1} and \ref{fig:6804_freq_2}. It is worth highlighting that, but for the offsets, frequency signals are very similar to the tuned PI controller as indicated by the dotted lines.

\subsection{Control Effort}
The optimally tuned PI controller demonstrated a maximum absolute control effort that was equal to or less than that for all experiments for Areas 1 and 2. Despite this result, on average the PI controller used more control effort throughout the simulation. In particular, the OU Noise, Baseline, and Priority Experience Replay agents out performed the PI controller for this metric.

\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7205_ctl_effort_comparison/ctl_effort_comparison.tikz}}
		\caption[Maximum absolute control effort comparison]{Maximum control effort for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}\label{fig:7205_max_ctl_effort}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7206_ctl_effort_comparison/ctl_effort_comparison.tikz}}
		\caption[Average absolute control effort comparison]{Average control effort for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}\label{fig:7205_avg_ctl_effort}
	\end{minipage}
\end{figure}

Figures \ref{fig:7205_max_ctl_effort} and \ref{fig:7205_avg_ctl_effort} show the maximum and average control effort for Area 1 given a +0.01pu step change disturbance at the 15 sec mark, respectively.

A visual inspection of the control signal for the OU Noise experiment highlights that the control signal is very similar to that of the optimally tuned PI controller. The principal difference between PI and OU Noise control signals are the Area 1 signal being too low, and the Area 2 signal being too high. Furthermore, there is a small deviation in the control signal prior to the disturbance which results in unnecessary frequency deviations. The control signals in Areas 1 and 2 for the OU Noise experiment given a +0.01pu step change disturbance at the 15 sec mark are shown in Figures \ref{fig:6503_ctl_1} and \ref{fig:6505_ctl_2}, respectively.

The similarity in the PI and Stochastic Load Demand control signals are even more pronounced. Notably, after the step change disturbance, the control signal tracks the PI control signal very closely. The principal different is prior to the step change disturbance where the Stochastic Load Demand control signal demonstrates some oscillatory behaviour resulting in unnecessary frequency disturbance. The control signals in Areas 1 and 2 for the Stochastic Load Demand experiment given a +0.01pu step change disturbance at the 15 sec mark are shown in Figures \ref{fig:6803_ctl_1} and \ref{fig:6805_ctl_2}, respectively.


\subsection{Settling Time}
The PI controller outperformed the Baseline, OU Noise, Activation Function, and Priority Experience Replay experiments for settling times under all scenarios for Areas 1 and 2.

\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7207_frequency_comparison/macr100_comparison.tikz}}
		\caption[Area 1 settling time comparison]{Trained agent settling time in Area 1 for the +0.01pu step change disturbance at the 15 sec mark.}\label{fig:7207_settling_time_area_1}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7208_macr100_comparison/macr100_comparison.tikz}}
		\caption[Area 2 settling time comparison]{Trained agent settling time in Area 2 for the +0.01pu step change disturbance at the 15 sec mark.}\label{fig:7208_settling_time_area_2}
	\end{minipage}
\end{figure}

For selected scenarios the Expert Learner, and Neural Network experiments were able to obtain better settling times that the PI Controller. The Stochastic Load Demand experiment outperformed the PI controller under all cases.

The Baseline, OU Noise, Activation Function, and Priority Experience Replay experiments were not able to develop control policies that dampened frequency signals sufficiently to qualify for the settling time definition outlined in \textsection \ref{sec:agent_performance}. This meant that a settling time could not be obtained for these agents. The Stochastic Load Demand agent, and to a lesser degree Expert Learner and Neural Network agents, were often able to demonstrate settling times that were half that of the PI controller.

Figures \ref{fig:7207_settling_time_area_1} and \ref{fig:7208_settling_time_area_2} show the settling times for Areas 1 and 2 given a +0.01pu step change disturbance.

\section{Explanations for DDPG Under-performance}
The DDPG trained neural network was unable to outperform the PI controller for the load frequency control problem. Although it is beyond the scope of this thesis to critically analyse the reasons why this occurred, some possible explanations for the observation are as follows:
\begin{itemize}
	\item \textbf{Poor reward structure:} the reward function selection may result in a complicated and highly variable cost manifold when applied to the load frequency control problem. The selected reward function structure should theoretically allow for an optimisation algorithm to find a globally optimised control strategy that allows the neural network to move the episodic cumulative reward towards zero. For a given reward function, however simple, it is difficult to anticipate if there will be many deep local minimums that prevent the globally optimal control solution being found.
	\item \textbf{Small neural network architecture:} the neural network architecture may not be big enough to provide for fine control adjustment required to reduce the observed frequency offsets to zero. A larger network may be able to provide finer adjustments allowing the neural network controller to reduce offsets; however, the trade off is longer training times.
	\item \textbf{Incorrect software implementation:} the software stack for DDPG, neural networks, and environment simulation is complicated and dependent on many existing libraries. Despite verification works to provide reasonable assurance developed software was working as intended, a programming error could result in the DDPG algorithm being unable to converge on an optimal control solution. 
\end{itemize}
