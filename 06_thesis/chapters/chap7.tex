\chapter{Analysis and Discussion}

This chapter provides analysis for both agent training and trained agent performance phases of the simulation experiments. For agent training, this will include the comparison of final and highest MACR100 scores for the different simulation experiments. Explanations are offered where an agent performance is poor. Similarly, a comparison of trained agent performance from the different experiments is provided, as well as a comparison to an optimally trained PI controller. Explanations are offered for instances where trained agent performance does not exceed the tuned PI controller.

\section{Training}
Monotonically increasing cumulative reward values indicate a continuous average increase in deep reinforcement learning agent performance. This is a desirable feature for cumulative reward plots. Plots of the MACR100 evolution for each experiment are detailed in Figures \ref{fig:6201_average_reward}, \ref{fig:6301_average_reward}, \ref{fig:6401_average_reward}, \ref{fig:6501_average_reward}, \ref{fig:6601_average_reward}, \ref{fig:6701_average_reward}, and \ref{fig:6801_average_reward}.

Generally, it can be observed that MACR100 is positively correlated with the episode number for all experiments. It is noted that the MACR100 signals were not monotonic for any experiment. Moreover, the MACR100 plots feature small to large fluctuations in the signal with increase in episode. This is especially true for the neural network experiment, expert learner experiment, and stochastic load demand experiment.

During the early stages of training all experiments demonstrated a period during which rapid learning took place. This is indicated by a steep rise in the MACR100 score, which typically took place in the first 1000 episodes of training, as shown in Figure \ref{fig:7101_MACR100_first}. In all experiments the agents demonstrated policy convergence, as indicated in reduced fluctuations of the MACR100 score. Convergence typically took place after the episode 7000, as shown in Figure \ref{fig:7102_MACR100_second}.

\clearpage

\begin{figure}[h]
	\centering
	
	\input{./figures/7101_learning_comparison/learning_comparison_plot.tikz}
	\caption{Comparison of the MACR100 evolution for all experiments during the first 5000 episodes of training.}\label{fig:7101_MACR100_first}
	
	\vspace{0.5cm}
	
	\input{./figures/7102_learning_comparison/learning_comparison_plot.tikz}
	\caption{Comparison of the MACR100 evolution for all experiments during the final 5000 episodes of training.}\label{fig:7102_MACR100_second}
\end{figure}

\clearpage

The baseline experiment demonstrated the best MACR100 score at the 10000 episode, and one of the highest MACR100 scorces overall. The exception was the expert learner experiment which yielded a marginally higher score than the baseline during training. Figure \ref{fig:7103_macr100_comparison} provides a comparison of the MACR100 for different experiments at episode 10000. Figure \ref{fig:7104_macr100_comparison} provides a comparison of the highest MACR100 achieved during training for each experiment. The principal observation that can be made from these results is that the original DDPG hyperparameters and neural network architecture used by Lillicrap \textit{et alias} are the best suited to the load frequency control task.

\begin{figure}[h]
	\begin{minipage}{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7103_macr100_comparison/macr100_comparison.tikz}}
		\caption{MACR100 at episode 10000.}\label{fig:7103_macr100_comparison}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7104_macr100_comparison/macr100_comparison.tikz}}
		\caption{Best MACR100 achieved during training.}\label{fig:7104_macr100_comparison}
	\end{minipage}
\end{figure}
 
\section{Trained Agent Performance}

\clearpage

\subsection{Cumulative Reward}

\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7201_cum_reward_comparison/cum_reward_comparison.tikz}}
		\caption{Trained agent cumulative reward for a +0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7202_cum_reward_comparison/cum_reward_comparison.tikz}}
		\caption{Trained agent cumulative reward for a -0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
\end{figure}

\clearpage

\subsection{Frequency Deviations}

\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7203_frequency_comparison/frequency_comparison.tikz}}
		\caption{Maximum frequency deviation for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7204_frequency_comparison/frequency_comparison.tikz}}
		\caption{Average frequency deviation for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
\end{figure}

\clearpage

\subsection{Control Effort}

\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7205_ctl_effort_comparison/ctl_effort_comparison.tikz}}
		\caption{Maximum control effort for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7206_ctl_effort_comparison/ctl_effort_comparison.tikz}}
		\caption{Average control effort for the trained agent in Area 1 during the +0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
\end{figure}

\clearpage

\subsection{Settling Time}

\begin{figure}[h]
	\begin{minipage}[t]{0.50\textwidth}
		\centering
		\resizebox{7cm}{!}{\input{./figures/7207_frequency_comparison/macr100_comparison.tikz}}
		\caption{Trained agent settling time in Area 1 for the +0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
	\hspace{0.25cm}
	\begin{minipage}[t]{0.50\textwidth}
		\resizebox{7cm}{!}{\input{./figures/7208_macr100_comparison/macr100_comparison.tikz}}
		\caption{Trained agent settling time in Area 2 for the +0.01pu step change disturbance at the 15 sec mark.}
	\end{minipage}
\end{figure}

Monotonically increasing curves in subplot (a) and (b) are desirable, and indicate stable learning for a given reward function, and set of hyperparameters

Reward functions are designed to achieve desirable control behaviour from the agent. The task of controlling frequency and power interchange for a two area power system should therefore have a reward structure that will incentivise the agent to minimise frequency deviations and minimise deviations in the tie-line power interchange from scheduled values. Moreover, the agent should be discouraged from letting the system see large frequency excursions that would activate system protection settings in a real world power system. Finally, it would be useful if the agent develops policies that meet the objectives outlined above, without requiring acute control signals.

Whilst there are no restrictions on reward function specification, it is widely acknowledged that poor reward function selection can impede agent learning, or produce undesirable behaviour. Evidence of unstable and divergent learning can be observed in experiment A and B, as shown in subplot (a) and (b), in Figure \ref{fig:results_A} and \ref{fig:results_B}, respectively. This resulted in the DDPG agent developing poor control policies that saturated control actions, as shown in subplot (e) and (f). Agent frequency control performance saw unacceptably large deviations from scheduled values, as shown in subplot (c) and (d).

Experiments C, D, and E removed early termination conditions and associated penalties, resulting in an improvement to agent learning stability, as shown in subplot (a) and (b), in Figure \ref{fig:results_C}, \ref{fig:results_D}, and \ref{fig:results_E}, respectively. Control policies developed under these conditions no longer saturated control actions, as shown in subplot (e) and (f). This led to an improvement in agent control performance, comparable to the optimally tuned PI controller, shown in subplot (c) and (d).

After reviewing agent performance during training it was observed that control signals in experiments A to E were noisy. OU noise is added to control signals in order to help the DDPG agent explore the policy and state-action value spaces to reduce the probability of convergence on sub-optimal policies; however, excessive noise settings have been shown to impede agent learning during the later stages of training \cite{}. Reducing the OU noise parameters by a factor of 10 in experiment F saw agent frequency control performance closely resemble the frequency control performance of the optimally tuned PI controller, shown in subplot (c) and (d), for Figure \ref{fig:results_F}. This result shows the feasibility for using DDPG trained neural networks in controlling the frequency of a two area power system.

This result needs to be tested further by making additional reward function modifications, and minor changes to noise processes. Further, investigation of these two aspects should be undertaken using a more rigorous and scientific approach. Research into experimental approaches for reward function, and hyperparameter testing would ensure that research is conducted in a systematic providing better insight to the problem. Research should also be undertaken to determine a better approach to reporting agent learning, and measuring control performance of the trained agent.

Once settings for which DDPG learns and performs optimally are understood, experiments involving stochastic load demand and non-linear plant models should commence. Contact has been made with Territory Generation and Power and Water Corporation, with the understanding that stochastic load demand profile can be provided; however, the issues such as conditions under which the data can be used and the data sensitivity have yet to be discussed.